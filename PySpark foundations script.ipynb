{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab05ceb9-c258-4a22-bafe-1aa335d40b8f",
   "metadata": {},
   "source": [
    "# Process, analyze, and summarize data with Spark and PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e794cc26-aa69-4047-a690-2a342ebb524a",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "Did you know that a billion records are processed daily in PySpark by companies worldwide? As big data is on the rise, you’ll need tools like PySpark to process massive amounts of data.\n",
    "\n",
    "This guided project was designed to introduce data analysts and data science beginners to data analysis in PySpark. This 2-hour project course teaches you how to create a PySpark environment, explore and clean large data, aggregate and summarize data, and visualize data using real-life examples. By the end of this guided project, you’ll create a Jupyter Notebook that processes, analyzes, and summarizes data using PySpark. By working on hands-on tasks, you will gain a solid knowledge of data aggregation and summarization with PySpark, helping you acquire job-ready skills. \n",
    "\n",
    "You don’t need any experience in PySpark, but knowledge of Python is essential to succeeding in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156abaa0-8a56-4155-b441-18af5eeeacae",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "This project demonstrates how to process and analyze large datasets using PySpark, focusing on employees data. Tasks include data loading, cleaning, exploration, and aggregation, culminating in insights on employee salaries and demographics.\n",
    "\n",
    "### About the Dataset\n",
    "**employees.csv**: Contains employee details like employee numbers, names, birth dates, hire dates, etc.\n",
    "\n",
    "**updated_salaries.csv**: Contains salary details for each employee, including salary amounts and dates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd3c17-248d-4808-956f-21c01eff8e2d",
   "metadata": {},
   "source": [
    "# Task One: Set up and overview of the project\n",
    "In this task, you will get an overview of the project and set up the PySpark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5cd815f-63b3-4e74-9b53-35237b7383da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import col, avg, max, min, countDistinct, sum, round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e15426d-f29a-44b3-9335-9e64b31a7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up PySpark environment\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ea312-498a-43f1-b68e-12d75abb805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the SparkSession\n",
    "\n",
    "\n",
    "## Verify that SparkSession is created\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777bfd46-805d-466f-b6f7-9e0db5113efa",
   "metadata": {},
   "source": [
    "# Task Two: Load the data\n",
    "In this task, you will load the employees.csv and updated_salaries.csv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3021b17b-9829-4cdb-921e-3c66d7744071",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the employees.csv dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6f5c39-3b6d-48f7-8196-64dd2f2216aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the updated_salaries.csv dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403c092-bf3f-473a-9c36-d468f6e37baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show the first few rows of the employees data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fabf1f-f9bf-4554-b65c-48771773c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show the first few rows of the salaries data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3950863-3d2f-49ea-bb9a-3586570f535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print the schema for employees data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f45eb2-beec-4e9e-9a1f-33d88c04841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print the schema for salary data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812d9ee-e0a8-49f2-b6bc-bbb4ab853818",
   "metadata": {},
   "source": [
    "# Task Three: Clean and process the data\n",
    "In this task, you will perform quick data cleaning by converting variables to proper data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae18cc0-f1f5-4b72-82fe-51995b4b3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "## Cast the 'emp_no' column in the employees data to a string\n",
    "\n",
    "\n",
    "## Print the updated schema\n",
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8470fe-fa25-45a9-a420-3c877db60a3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain transformations to cast 'emp_no' to string and 'to_date' & 'from_date' to date\n",
    "\n",
    "\n",
    "## Show the updated schema\n",
    "sal_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8b03eb-41e9-4b5b-9571-42e96b86cb15",
   "metadata": {},
   "source": [
    "# Task Four: Explore the data\n",
    "In this task, you will explore the salaries data by computing summary statistics and visualizing the salary column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9845e-704a-4f06-9d63-530fa0d7d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a sum of missing values per column in the salary data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55da30-a71a-46c3-9cfb-a94bf36a21ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create the summary statistics for the salary data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85285cb5-57f7-48fb-a55f-952981f18b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count total rows and unique employees in salary data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2fac36-e51e-4da2-a5ad-6ba362709bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the salary distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d48859-39f7-4967-85b6-53c08fa3568f",
   "metadata": {},
   "source": [
    "# Practice Activity One: Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6487ce1-b885-4ada-b44e-6fcf3bff48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a sum of missing values per column in the employees data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70795dbd-e31d-49b6-9c5b-8797a6fd17a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count the number of rows in the employees data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab36f36-0b27-4b94-9464-010b8bd73238",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many different first names can be found in the employees data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f0f7a-2a23-4ac8-b4b3-e16f8e89c312",
   "metadata": {},
   "source": [
    "# Task Five: Aggregate and summarize the data\n",
    "In this task, you will perform data aggregation and summarization using the salaries data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bebf7cb-6e44-423d-80ff-b2da279c8142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Group the data and calculate the average salary for each department\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ae615-4034-4114-82e7-d3f0a9dffe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the average salary and number of employees in each department\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7a016-d1f4-41fa-88ba-ae9765bc9ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert Spark data frame  to Pandas for visualization\n",
    "dept_summary_df = dept_summary.toPandas()\n",
    "\n",
    "## Plot the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f13853-b247-4493-a892-785be265fded",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Retrieve a list of employee numbers and the average salary.\n",
    "## Make sure that you return where the average salary is more than $120,000\n",
    "\n",
    "## Group by employee number and calculate the average salary\n",
    "\n",
    "\n",
    "## Order in descending order of average_salary\n",
    "\n",
    "\n",
    "## Show the result\n",
    "emp_avg_salary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa860eae-8548-415c-bc52-e6ace6028f16",
   "metadata": {},
   "source": [
    "# Task Six: Join the data sets\n",
    "In this task, you will join the salaries and employees data using the employees number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ace7be7-7f65-4f3f-a0ab-eabbedb8f025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "## Create an age column in the employees data\n",
    "## Age when the employee was hired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692b74e-bfb3-4ad4-9917-0627efa3870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join salaries and employees data on 'emp_no'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d3a34-614e-4fb8-bf6c-e2bd2ce551e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve a list of employee numbers and the average salary.\n",
    "## Make sure that you return where the average salary is more than $120,000\n",
    "\n",
    "## Group by employee number and calculate the average salary\n",
    "\n",
    "\n",
    "## Join the aggregated result back with the original employee data to get first_name, last_name, hire_date\n",
    "\n",
    "\n",
    "\n",
    "emp_salary_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a831e-ba55-4e3b-93d8-e7de726d9091",
   "metadata": {},
   "source": [
    "# Cumulative Activity: Analyze employees' retention\n",
    "\n",
    "As a junior data analyst at a growing company, you are tasked with analyzing employee retention. Your aim is to find departments with the highest amount of employees that have worked longer than ten years. This will assist HR in enhancing employees' engagement and retention strategies. \n",
    "\n",
    "To complete this activity, you will use the employee dataset and create a data frame with the employee totals in each department for a period over 10 years (calculated by from_date and to_date).  Finally, you'll visualize how long-term employees are spread across departments via a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d89db-5b94-4f43-9501-e53977ffbc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "## Calculate the years worked based on the difference between 'to_date' and 'from_date'\n",
    "\n",
    "## Group by emp_no and dept_no to sum the years worked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade01c7-f7ba-4d51-b00b-3c1c02c51399",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter employees who have worked more than 10 years\n",
    "\n",
    "\n",
    "## Group by department and count distinct employees who worked more than 10 years\n",
    "\n",
    "\n",
    "## Show the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72289f86-e758-4777-bd40-8524ecd38c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the Spark data frame to Pandas for visualization\n",
    "\n",
    "\n",
    "## Create a bar chart to visualize the distribution of long-term employees across departments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ab51f-d191-41d0-b846-214ffea74d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
